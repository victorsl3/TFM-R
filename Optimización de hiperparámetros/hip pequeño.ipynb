{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\anaconda3\\envs\\transformers\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import gc\n",
    "\n",
    "from sklearn.datasets import load_iris, load_diabetes, load_digits, load_wine, \\\n",
    "    load_breast_cancer, fetch_california_housing, fetch_covtype\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from EDAspy.optimization import EBNA\n",
    "from EDAspy.optimization import plot_bn\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='Probability values don\\'t exactly sum to 1.*')\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris, load_diabetes, load_digits, load_wine, load_breast_cancer, fetch_california_housing, fetch_covtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def load_dataset(name, return_as_pandas=False, subset_ratio=1.0):\n",
    "    datasets = {\n",
    "        'iris': load_iris,\n",
    "        'diabetes': load_diabetes,\n",
    "        'digits': load_digits,  # es de imagenes pero como son 8x8 lo vamos a dejar como si fuera tabular\n",
    "        'wine': load_wine,\n",
    "        'breast_cancer': load_breast_cancer,\n",
    "        'california_housing': fetch_california_housing,\n",
    "        'forest_covertypes': fetch_covtype,\n",
    "        # 'olivetti_faces': fetch_olivetti_faces,\n",
    "        # 'lfw_people': fetch_lfw_people\n",
    "    }\n",
    "\n",
    "    if name not in datasets:\n",
    "        raise ValueError(f\"Dataset '{name}' no disponible.\")\n",
    "\n",
    "    data = datasets[name]()\n",
    "\n",
    "    X, y = data.data, data.target\n",
    "\n",
    "    # Reducir el tamaño del conjunto de datos si subset_ratio es menor que 1\n",
    "    if subset_ratio < 1.0:\n",
    "        total_size = int(subset_ratio * len(X))\n",
    "        indices = np.random.choice(len(X), total_size, replace=False)\n",
    "        X = X[indices]\n",
    "        y = y[indices]\n",
    "\n",
    "    # Dividir el conjunto de datos en entrenamiento, validación y prueba\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
    "\n",
    "    if return_as_pandas:\n",
    "        # Combinar los conjuntos en un solo DataFrame\n",
    "        import pandas as pd\n",
    "        train_df = pd.DataFrame(X_train)\n",
    "        train_df['target'] = y_train\n",
    "        train_df['set'] = 'train'\n",
    "\n",
    "        val_df = pd.DataFrame(X_val)\n",
    "        val_df['target'] = y_val\n",
    "        val_df['set'] = 'validation'\n",
    "\n",
    "        test_df = pd.DataFrame(X_test)\n",
    "        test_df['target'] = y_test\n",
    "        test_df['set'] = 'test'\n",
    "\n",
    "        full_df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "        return full_df\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_model(input_dim, output_dim, hparams):\n",
    "    class CustomNN(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, hparams):\n",
    "            super(CustomNN, self).__init__()\n",
    "            self.layers = nn.ModuleList()\n",
    "            self.dropout_rates = []  # Para almacenar las tasas de dropout\n",
    "\n",
    "            layer_dims = [int(x) for x in hparams['rec_hidden_layers_config'].split('_')]\n",
    "            prev_dim = input_dim\n",
    "\n",
    "            for layer_dim in layer_dims:\n",
    "                layer = nn.Linear(prev_dim, layer_dim)\n",
    "                self.layers.append(layer)\n",
    "                prev_dim = layer_dim\n",
    "\n",
    "                # Verificar y almacenar la tasa de dropout si se especifica\n",
    "                if 'dropout' in hparams['regularization']:\n",
    "                    dropout_rate = float(hparams['regularization'].split('_')[1])\n",
    "                    self.dropout_rates.append(dropout_rate)\n",
    "                else:\n",
    "                    self.dropout_rates.append(0)  # No dropout\n",
    "\n",
    "            self.output_layer = nn.Linear(prev_dim, output_dim)\n",
    "\n",
    "        def forward(self, x):\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                x = layer(x)\n",
    "                # Aplicar función de activación común a toda la red\n",
    "                if hparams['activation'] == 'relu':\n",
    "                    x = F.relu(x)\n",
    "                elif hparams['activation'] == 'tanh':\n",
    "                    x = torch.tanh(x)\n",
    "                elif hparams['activation'] == 'sigmoid':\n",
    "                    x = torch.sigmoid(x)\n",
    "                elif hparams['activation'] == 'leaky_relu':\n",
    "                    x = F.leaky_relu(x)\n",
    "                elif hparams['activation'] == 'elu':\n",
    "                    x = F.elu(x)\n",
    "\n",
    "                # Aplicar dropout según la configuración\n",
    "                if self.dropout_rates[i] > 0:\n",
    "                    x = F.dropout(x, p=self.dropout_rates[i], training=self.training)\n",
    "\n",
    "            x = self.output_layer(x)\n",
    "            return F.log_softmax(x, dim=1)\n",
    "\n",
    "    return CustomNN(input_dim, output_dim, hparams)\n",
    "\n",
    "\n",
    "\n",
    "def initialize_frequency_for_combined(possible_values):\n",
    "    frequency = {var: [1/len(possible_values[var])] * len(possible_values[var]) for var in possible_values}\n",
    "    return frequency\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(hparams, input_dim, output_dim, X_train, y_train, X_val, y_val):\n",
    "    # Verificar si CUDA está disponible y configurar el dispositivo\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Construir el modelo y moverlo al dispositivo apropiado (GPU si está disponible)\n",
    "    model = build_model(input_dim, output_dim, hparams).to(device)\n",
    "\n",
    "    # Seleccionar el optimizador basado en los hiperparámetros\n",
    "    if hparams['optimizer'] == 'adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=hparams['learning_rate'])\n",
    "    elif hparams['optimizer'] == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=hparams['learning_rate'])\n",
    "    elif hparams['optimizer'] == 'momentum':\n",
    "        # Nota: 'momentum' no es un optimizador en sí, pero se puede implementar con SGD y un valor de momento.\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=hparams['learning_rate'], momentum=0.9)\n",
    "    elif hparams['optimizer'] == 'rmsprop':\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr=hparams['learning_rate'])\n",
    "    elif hparams['optimizer'] == 'adamw':\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=hparams['learning_rate'])\n",
    "\n",
    "    # Definir el criterio de pérdida\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    if isinstance(X_train, np.ndarray):\n",
    "            X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    else:\n",
    "        X_train = X_train.to(device)\n",
    "\n",
    "    if isinstance(y_train, np.ndarray):\n",
    "        y_train = torch.tensor(y_train, dtype=torch.int64).to(device)\n",
    "    else:\n",
    "        y_train = y_train.to(device)\n",
    "\n",
    "    if isinstance(X_val, np.ndarray):\n",
    "        X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    else:\n",
    "        X_val = X_val.to(device)\n",
    "\n",
    "    if isinstance(y_val, np.ndarray):\n",
    "        y_val = torch.tensor(y_val, dtype=torch.int64).to(device)\n",
    "    else:\n",
    "        y_val = y_val.to(device)\n",
    "\n",
    "    # Preparar los DataLoader para entrenamiento y validación\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=hparams['batch_size'], shuffle=True)\n",
    "\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    val_loader = DataLoader(dataset=val_dataset, batch_size=hparams['batch_size'], shuffle=False)\n",
    "\n",
    "    # # Entrenamiento del modelo\n",
    "    # model.train()\n",
    "    # for epoch in tqdm(range(hparams['epochs']), desc='Epochs'):\n",
    "    #     for X_batch, y_batch in tqdm(train_loader, leave=False, desc=f'Training Epoch {epoch + 1}'):\n",
    "    #         optimizer.zero_grad()\n",
    "    #         output = model(X_batch)\n",
    "    #         loss = criterion(output, y_batch)\n",
    "    #         loss.backward()\n",
    "    #         optimizer.step()\n",
    "\n",
    "    # Entrenamiento del modelo\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(hparams['epochs']), desc='Epochs'):\n",
    "        for X_batch, y_batch in tqdm(train_loader, leave=False, desc=f'Training Epoch {epoch + 1}'):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluación del modelo\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in tqdm(val_loader, desc='Validation'):\n",
    "            output = model(X_batch)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "\n",
    "    # Liberar memoria de CUDA\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "        del model, X_train, y_train, X_val, y_val\n",
    "        torch.cuda.empty_cache()\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def hyperparameter_search(dataset_name, hyperparameters, size_gen=20, max_iter=20, dead_iter=5,alpha=0.5,output_file=\"\", subset_ratio=1.0):\n",
    "    # Cargar los datos\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = load_dataset(name=dataset_name, subset_ratio=subset_ratio)\n",
    "\n",
    "    # Convertir los datos a tensores de PyTorch si aún no lo son\n",
    "    # Verificar y convertir X_train, X_val, X_test si es necesario\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float) if not torch.is_tensor(X_train) else X_train\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float) if not torch.is_tensor(X_val) else X_val\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float) if not torch.is_tensor(X_test) else X_test\n",
    "\n",
    "    # Verificar y convertir y_train, y_val, y_test si es necesario\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long) if not torch.is_tensor(y_train) else y_train\n",
    "    y_val = torch.tensor(y_val, dtype=torch.long) if not torch.is_tensor(y_val) else y_val\n",
    "    y_test = torch.tensor(y_test, dtype=torch.long) if not torch.is_tensor(y_test) else y_test\n",
    "\n",
    "    # Preparar los datos para PyTorch\n",
    "    input_dim = X_train.shape[1]  # Dimensiones de entrada\n",
    "    all_labels = torch.cat((y_train, y_val, y_test))  # Ahora todos son tensores de PyTorch\n",
    "    unique_labels = torch.unique(all_labels)\n",
    "\n",
    "    # Ajustar las etiquetas si el valor mínimo es 1\n",
    "    if all_labels.min() == 1:\n",
    "        y_train -= 1\n",
    "        y_val -= 1\n",
    "        y_test -= 1\n",
    "        print(\"Las etiquetas han sido ajustadas para empezar en 0.\")\n",
    "        # Recalcular las etiquetas únicas después del ajuste\n",
    "        all_labels = torch.cat((y_train, y_val, y_test))\n",
    "        unique_labels = torch.unique(all_labels)\n",
    "\n",
    "    output_dim = len(unique_labels)  # Determinar la cantidad de clases únicas\n",
    "    print(unique_labels)\n",
    "\n",
    "    # Configuración de EBNA (sin cambios)\n",
    "    possible_values_numeric = {i: hyperparameters[var] for i, var in enumerate(hyperparameters)}\n",
    "    frequency_numeric = initialize_frequency_for_combined(possible_values_numeric)\n",
    "    ebna = EBNA(\n",
    "        size_gen=size_gen,\n",
    "        max_iter=max_iter,\n",
    "        dead_iter=dead_iter,\n",
    "        n_variables=len(possible_values_numeric),\n",
    "        alpha=alpha,\n",
    "        possible_values=possible_values_numeric,\n",
    "        frequency=frequency_numeric\n",
    "    )\n",
    "\n",
    "    # Wrapper y ejecución de EBNA (sin cambios)\n",
    "    def multiKR_cost_wrapper_eda(solution_array):\n",
    "        hyperparameter_names = list(hyperparameters.keys())\n",
    "        solution_dict = {name: value for name, value in zip(hyperparameter_names, solution_array)}\n",
    "\n",
    "        # Conversión de tipos\n",
    "        solution_dict['learning_rate'] = float(solution_dict['learning_rate'])\n",
    "        solution_dict['epochs'] = int(solution_dict['epochs'])\n",
    "        solution_dict['batch_size'] = int(solution_dict['batch_size'])\n",
    "\n",
    "        # print(f\"Los hiperparámetros seleccionados son: {solution_dict}\")\n",
    "        accuracy = train_and_evaluate_model(solution_dict, input_dim, output_dim, X_train, y_train, X_val, y_val)\n",
    "        return -accuracy  # Negativo de la precisión\n",
    "\n",
    "    ebna_result = ebna.minimize(multiKR_cost_wrapper_eda)\n",
    "\n",
    "    # Antes de llamar a ebna.minimize(multiKR_cost_wrapper_eda)\n",
    "    hyperparameter_names = list(hyperparameters.keys())\n",
    "\n",
    "\n",
    "\n",
    "    # Suponiendo que hyperparameter_names es una lista de nombres en el mismo orden que el modelo los utiliza\n",
    "    index_to_name = {i: name for i, name in enumerate(hyperparameter_names)}\n",
    "\n",
    "    # Obtener la estructura de la red y convertir los índices a nombres\n",
    "    arcs = ebna.pm.print_structure()  # Asumiendo que esto devuelve una lista de tuplas como [(0, 1), (2, 3), ...]\n",
    "    arcs_with_names = []\n",
    "\n",
    "    # Recorre todos los arcos y convierte los índices en nombres\n",
    "    for arc in arcs:\n",
    "        try:\n",
    "            # Convierte los índices a enteros si son cadenas\n",
    "            start_index = int(arc[0])\n",
    "            end_index = int(arc[1])\n",
    "\n",
    "            # Encuentra los nombres correspondientes y los agrega a la lista\n",
    "            arcs_with_names.append((index_to_name[start_index], index_to_name[end_index]))\n",
    "        except KeyError as e:\n",
    "            print(f\"Key error: {e}. This index does not exist in the hyperparameter names.\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Value error: {e}. The arc should be a tuple of integers.\")\n",
    "\n",
    "\n",
    "    # Ahora, al generar el gráfico, usa los arcos con nombres en lugar de índices numéricos\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(hyperparameter_names)\n",
    "    G.add_edges_from(arcs_with_names)\n",
    "\n",
    "    # Generar un layout para los nodos si _set_positions no funciona como se espera\n",
    "    pos = nx.circular_layout(G)\n",
    "\n",
    "    # Llama a plot_bn pasando 'pos' explícitamente\n",
    "    plot_bn(\n",
    "        arcs=arcs_with_names,\n",
    "        var_names=hyperparameter_names,\n",
    "        pos=pos,  # Pasa el layout generado como 'pos'\n",
    "        title=\"Estructura de la Red Bayesiana\", output_file=output_file)\n",
    "\n",
    "    return ebna_result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_accuracy_evolution(ebna_result, dataset_name=\"\"):\n",
    "    # Convertir los valores negativos de precisión a positivos, ya que se minimizó el negativo de la precisión\n",
    "    accuracy_values = [-x for x in ebna_result.history]\n",
    "\n",
    "    # Crear la figura y los ejes\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    # Título y etiquetas\n",
    "    plt.title('Mejor individuo por generación en '+ str(dataset_name))\n",
    "    plt.xlabel('Generación')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    # Corregir el rango del eje x para que comience en 1 y termine en el número de generaciones\n",
    "    generations = list(range(1, len(accuracy_values) + 1))\n",
    "\n",
    "    # Graficar la línea que muestra la mejora de la precisión\n",
    "    plt.plot(generations, accuracy_values, color='b', label='EBNA', marker='o')\n",
    "\n",
    "    # Añadir una marca de dato en cada punto de la línea\n",
    "    for i, acc in enumerate(accuracy_values, start=1):\n",
    "        plt.annotate(f'{acc:.4f}', (i, acc), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "    # Mostrar la leyenda\n",
    "    plt.legend()\n",
    "\n",
    "    # Ajustar los límites del eje x para mejorar la presentación\n",
    "    plt.xlim(0.5, len(accuracy_values) + 0.5)\n",
    "\n",
    "    # Mostrar la gráfica\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "# Clasificando los conjuntos de datos en tres categorías: multiclase, binario y regresión\n",
    "datasets_multiclase = [\n",
    "    'iris',\n",
    "    'digits',\n",
    "    'wine',\n",
    "    'forest_covertypes'\n",
    "]\n",
    "\n",
    "datasets_binario = [\n",
    "    'breast_cancer'\n",
    "]\n",
    "\n",
    "datasets_regresion = [\n",
    "    'diabetes',\n",
    "    'california_housing'\n",
    "]\n",
    "\n",
    "datasets_multiclase, datasets_binario, datasets_regresion\n",
    "\n",
    "# %%\n",
    "from itertools import product\n",
    "\n",
    "# Define las opciones de neuronas por capa y el número máximo de capas\n",
    "neuron_options = ['32','64', '128', '256', '512', '1024', '2048']\n",
    "max_layers = 7\n",
    "\n",
    "# Generar todas las posibles configuraciones de capas\n",
    "layer_configurations = []\n",
    "for num_layers in range(1, max_layers + 1):\n",
    "    for combination in product(neuron_options, repeat=num_layers):\n",
    "        # Convertir la tupla de combinación en una cadena de configuración\n",
    "        config_string = '_'.join(combination)\n",
    "        layer_configurations.append(config_string)\n",
    "\n",
    "\n",
    "# %%\n",
    "len(layer_configurations)\n",
    "\n",
    "# %%\n",
    "layer_configurations[-1]\n",
    "\n",
    "# %%\n",
    "hyperparameters = {\n",
    "        'rec_hidden_layers_config': layer_configurations,\n",
    "        'activation': ['relu', 'tanh', 'sigmoid', 'leaky_relu', 'elu'],\n",
    "        'regularization': ['none', 'dropout_0.1', 'dropout_0.2', 'dropout_0.5'],\n",
    "        'optimizer': ['sgd', 'momentum', 'adam', 'rmsprop', 'adamw'],\n",
    "        'learning_rate': [0.1, 0.01, 0.001, 0.0005, 0.0001, 0.00005, 0.00001],\n",
    "        'epochs': [10, 20, 50, 100, 200],\n",
    "        'batch_size': [16, 32, 64, 128, 256, 512]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forest_covertypes\n",
      "Las etiquetas han sido ajustadas para empezar en 0.\n",
      "tensor([0, 1, 2, 3, 4, 5, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 50/50 [05:07<00:00,  6.15s/it]\n",
      "Validation: 100%|██████████| 227/227 [00:02<00:00, 107.16it/s]\n",
      "Epochs:  11%|█         | 22/200 [15:10<2:02:44, 41.38s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m dataset_name\u001b[38;5;241m=\u001b[39mdatasets_multiclase[\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataset_name)\n\u001b[1;32m----> 4\u001b[0m ebna_result \u001b[38;5;241m=\u001b[39m \u001b[43mhyperparameter_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize_gen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdead_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtresuno.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(ebna_result)\n\u001b[0;32m      6\u001b[0m plot_accuracy_evolution(ebna_result, dataset_name)\n",
      "Cell \u001b[1;32mIn[2], line 273\u001b[0m, in \u001b[0;36mhyperparameter_search\u001b[1;34m(dataset_name, hyperparameters, size_gen, max_iter, dead_iter, alpha, output_file, subset_ratio)\u001b[0m\n\u001b[0;32m    270\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m train_and_evaluate_model(solution_dict, input_dim, output_dim, X_train, y_train, X_val, y_val)\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39maccuracy  \u001b[38;5;66;03m# Negativo de la precisión\u001b[39;00m\n\u001b[1;32m--> 273\u001b[0m ebna_result \u001b[38;5;241m=\u001b[39m \u001b[43mebna\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmultiKR_cost_wrapper_eda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;66;03m# Antes de llamar a ebna.minimize(multiKR_cost_wrapper_eda)\u001b[39;00m\n\u001b[0;32m    276\u001b[0m hyperparameter_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(hyperparameters\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[1;32mc:\\Users\\victo\\anaconda3\\envs\\transformers\\lib\\site-packages\\EDAspy\\optimization\\eda.py:157\u001b[0m, in \u001b[0;36mEDA.minimize\u001b[1;34m(self, cost_function, output_runtime, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m t1 \u001b[38;5;241m=\u001b[39m process_time()\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_generation()\n\u001b[1;32m--> 157\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcost_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# select just one item to be the elite selection if first iteration\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39melite_temp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration[\u001b[38;5;241m0\u001b[39m, :]])\n",
      "File \u001b[1;32mc:\\Users\\victo\\anaconda3\\envs\\transformers\\lib\\site-packages\\EDAspy\\optimization\\eda.py:115\u001b[0m, in \u001b[0;36mEDA._check_generation_no_parallel\u001b[1;34m(self, objective_function)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_generation_no_parallel\u001b[39m(\u001b[38;5;28mself\u001b[39m, objective_function: \u001b[38;5;28mcallable\u001b[39m):\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluations \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_along_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\victo\\anaconda3\\envs\\transformers\\lib\\site-packages\\numpy\\lib\\shape_base.py:402\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[1;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m buff[ind0] \u001b[38;5;241m=\u001b[39m res\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m inds:\n\u001b[1;32m--> 402\u001b[0m     buff[ind] \u001b[38;5;241m=\u001b[39m asanyarray(\u001b[43mfunc1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43minarr_view\u001b[49m\u001b[43m[\u001b[49m\u001b[43mind\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, matrix):\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;66;03m# wrap the array, to preserve subclasses\u001b[39;00m\n\u001b[0;32m    406\u001b[0m     buff \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39m__array_wrap__(buff)\n",
      "Cell \u001b[1;32mIn[2], line 270\u001b[0m, in \u001b[0;36mhyperparameter_search.<locals>.multiKR_cost_wrapper_eda\u001b[1;34m(solution_array)\u001b[0m\n\u001b[0;32m    267\u001b[0m solution_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(solution_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    269\u001b[0m \u001b[38;5;66;03m# print(f\"Los hiperparámetros seleccionados son: {solution_dict}\")\u001b[39;00m\n\u001b[1;32m--> 270\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msolution_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39maccuracy\n",
      "Cell \u001b[1;32mIn[2], line 183\u001b[0m, in \u001b[0;36mtrain_and_evaluate_model\u001b[1;34m(hparams, input_dim, output_dim, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[0;32m    181\u001b[0m         output \u001b[38;5;241m=\u001b[39m model(X_batch)\n\u001b[0;32m    182\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(output, y_batch)\n\u001b[1;32m--> 183\u001b[0m         \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Evaluación del modelo\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\victo\\anaconda3\\envs\\transformers\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\victo\\anaconda3\\envs\\transformers\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset_name=datasets_multiclase[3]\n",
    "print(dataset_name)\n",
    "ebna_result = hyperparameter_search(dataset_name, hyperparameters, size_gen=5, max_iter=150, dead_iter=10,alpha=0.3, output_file=\"tresuno.png\", subset_ratio=1)\n",
    "print(ebna_result)\n",
    "plot_accuracy_evolution(ebna_result, dataset_name)\n",
    "\n",
    "# Abrir un archivo de texto en modo de escritura\n",
    "with open(\"resultado_ebna.txt\", \"w\") as file:\n",
    "    file.write(str(ebna_result))  # Escribir el contenido de ebna_result en el archivo\n",
    "\n",
    "# %%\n",
    "# dataset_name=datasets_multiclase[3]\n",
    "# print(dataset_name)\n",
    "# ebna_result = hyperparameter_search(dataset_name, hyperparameters, size_gen=50, max_iter=150, dead_iter=10,alpha=0.8,output_file=\"tresdos.png\")\n",
    "# print(ebna_result)\n",
    "# plot_accuracy_evolution(ebna_result, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
